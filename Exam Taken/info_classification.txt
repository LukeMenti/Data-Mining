%dopo bbegin\document inserire sta roba

\noindent
\hrulefill
\begin{flushright}
\textbf{University  of  Padua -  Department of Physics and Astronomy }\\

\textbf{Degree course:} Physics of Data\\
\textbf{Course:} Data Mining \\
\textbf{Year:} 2022-23\\
\textbf{Professor in charge:} A. Guolo\\
\end{flushright}
%\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
\
Luca Menti - 2063594 - \url{luca.menti@studenti.unipd.it} \\

\begin{verbatim} Exam's date: 27/06/2023\end{verbatim} \hrulefill
%\bigksip
\begin{center}
    \LARGE{\textbf{ Data Mining Report - Exam}}
\end{center}
    \hrulefill

    \section{Aim of the Report }
 The goal of this report is to analyze a genuine dataset and identify the most suitable model that explains the data. To accomplish this objective, I plan to first study a subset of the dataset, followed by the entire set. Utilizing various techniques, I will then determine which approach yields the most precise results for my analysis.

 \section{Analysis Techniques }
 \subsection{Point 1}
 \subsubsection{Logistic Regression}
Logistic regression is a statistical analysis method used to predict a binary outcome based on prior observations of a data set. It is a type of regression analysis and is a commonly used algorithm for solving binary classification problems. Logistic regression models a dependent data variable by analyzing the relationship between one or more existing independent variables. The purpose of logistic regression is to streamline the mathematics for measuring the impact of multiple variables with a given outcome. Logistic regression is used to calculate the probability of a binary event occurring and to deal with issues of classification. It can be used to predict whether a tumor is benign or malignant, if an incoming email is spam or not spam, if a credit card transaction is fraudulent or not fraudulent, or if a given user (or group of users) will buy a certain product or not. Logistic regression is represented similar to how linear regression is defined using the equation of a straight line. The most common logistic regression models a binary outcome, such as yes or no.

\subsubsection{Backward Selection}
 We start with all the variables in the model; we remove the variables with the largest p-value, one by one. Go on until the remaining covariates have a small $p-value$. The procedure canâ€™t be used when $p > n$.


 \subsubsection{Linear Discriminant Analysis}
Linear Discriminant Analysis (LDA) is a statistical analysis method used for classification tasks in machine learning. LDA works by finding a linear combination of features that best separates the classes in a dataset. It is used to calculate the probability of a binary event occurring and to deal with issues of classification. LDA is used in various applications, such as face recognition, medical diagnosis, and data visualization. 


 \subsubsection{Quadratic Discriminant Analysis}
Quadratic Discriminant Analysis (QDA) is a statistical analysis method used for classification tasks in machine learning. It is a variant of Linear Discriminant Analysis (LDA) that allows for non-linear separation of data.  LDA works by finding a linear combination of features that best separates the classes in a dataset. In contrast, QDA assumes that each class has its own covariance matrix, and the predictor variables are not assumed to have common variance across each of the k levels in Y. QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix is clearly untenable.
 
 \subsubsection{Generalized additive models (GAM)}
 In Gam we have flexibility in predicting Y using p covariates. Furthermore
non-linear functions of the covariates are allowed and the additivity of the components is maintained.
We also have applicability outside linear models.
The prons are:
\begin{itemize}
    \item{we can model non-linear relationships between $Y$ and $X_{j}$;}
\item{predictions can potentially be more accurate than those from a linear model;}
\item{we can still examine the effect of each $X_{j}$ on $Y$ individually, while holding all of the other variables fixed, as in the linear model;}
\item{ the smoothness of the function $f_{j}$ for the variable $X_{j}$ can be summarized via degrees of freedom.}
\end{itemize}

While the cons:
\begin{itemize}
   \item{the model is additive;}
\item{in case of many variables, important interactions can be missed;}
\item{interaction functions of the form $f_{jk}(X_j,X_k)$ can be fit with different smoothers (it can be complex).}
\end{itemize}

\subsection{Point 2}
 \subsubsection{Ridge Regression}

 With high-dimensional data ( $p$ very large w.r.t $n$ or even $p>n$ ) the maximum likelihood estimates can be difficult to calculate, it   can be not unique, it can have a large associated standard error and in general there are problems of identifiability and efficiency. One possible solution are shrinkage methos and one of the is
 Ridge Regression. 
 Shrinkage methods :
 \begin{itemize}
     \item are useful to regularize the estimation process;

     \item they shrink the estimates of the coefficients towards zero;

     \item in this way, the fitting of the model is improved the variability associated to the estimates is smaller.
 \end{itemize}

\subsection{Point 2}

 In Ridge regression there is a small shrinkage penalty when the coefficients  of model $\beta_j$ are close to zero. In particular for $\lambda=0 $  the penalty term has no effect, ridge regression will produce the least squares estimates, while for $\lambda \longrightarrow + \infty $ the impact of the shrinkage penalty grows, ridge regression coefficient estimates will approach zero. The choice of $\lambda$ is a crucial point in penalized regression. If $\lambda$ is too small there is no substantial penalization and the estimate is close to least squares estimate. If $\lambda$ is too large there is too much penalization and the estimates are shrunk to zero.
A good value of $\lambda$ is chosen typically using cross validation.

\subsubsection{Lasso}
Lasso is a recent alternative to ridge regression which does not select variables. There is a small shrinkage penalty if $\beta_j$ are close to zero or even equal to zero. Furthermore, Lasso performs variable selection as it forces some of the coefficient
estimates to be zero for sufficiently large $\lambda$ and it is easier to interpret than ridge regression. 
By the way the disadvantage is that the estimates are not in closed form, neither are the
associated variances.

\subsubsection{ Principal Component Analysis}

Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and data analysis. It is used to reduce the dimensionality of large datasets by transforming a large set of variables into a smaller one that still contains most of the information in the large set. PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. PCA creates variables that are linear combinations of the original variables. The new variables have the advantage of being uncorrelated and capturing the maximum amount of variation in the original dataset.  In conclusion, PCA is a linear dimensionality reduction technique that transforms a set of correlated variables into a smaller number of uncorrelated variables called principal components while retaining as much of the variation in the original dataset as possible.