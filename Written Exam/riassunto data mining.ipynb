{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93df155b",
   "metadata": {},
   "source": [
    "# Sum up\n",
    "\n",
    "## Linear models\n",
    "\n",
    "### Linear model aims\n",
    "\n",
    "Linear model are used to describe reality, to explain phenomena, to predict the behavior of variables.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "* Linear relation between X and Y\n",
    "* Independence of the residuals (no incr/decr trends in residual plots)\n",
    "* Homoscedasticity the residuals have the same level at each X\n",
    "* Normality on the residual of X and, by reflection on Y\n",
    "\n",
    "### What is P-value\n",
    "\n",
    "P-value is an indicator of the statistical significance of an hypothesis against another one in a statistical test.\n",
    "The p-value is a probability and ranges from 0 to 1. It is the probability, under the null hyp, of observing some more extreme events in the direction(s) of the alternative hypothesis. Therefore, when P is very close to 0, we have strong evidence to reject the null hypothesis. When it is near to 1, we have strong evidence in failing to reject the null hypothesis.\n",
    "The common significance thresholds are .01, .05, .1.\n",
    "The p value itself is not sufficient for conclusions. There is a chance that the sample chosen is not representing the entire population, maybe for a sampling error or a poor sample.\n",
    "\n",
    "### F test\n",
    "\n",
    "It's a statistical test and it's used to compare a model with no predictors (only intercept beta 0) with a model with different predictors.\n",
    "Null hypothesis: model without covariates is the same as model with covariates.\n",
    "How large does the F -statistic need to be before we can reject H 0 and\n",
    "conclude that there is a relationship? It turns out that the answer depends\n",
    "on the values of n and p. When n is large, an F -statistic that is just a\n",
    "little larger than 1 might still provide evidence against H 0 . In contrast,\n",
    "a larger F -statistic is needed to reject H 0 if n is small. When H 0 is true\n",
    "and the errors Îµ i have a normal distribution, the F -statistic follows an\n",
    "F -distribution.\n",
    "if we use the individual t-statistics and associated p-values in order\n",
    "to decide whether or not there is any association between the variables and\n",
    "the response, there is a very high chance that we will incorrectly conclude\n",
    "that there is a relationship. However, the F -statistic does not suffer from\n",
    "this problem because it adjusts for the number of predictors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It's more important than R squared because if the model is not statistically significant, R square is useless.\n",
    "If the model is significant but R square is low, the model is not so good but it is however useful.\n",
    "\n",
    "### Residual plot analysis\n",
    "\n",
    "To err is human, to err randomly is (statistically) divine.\n",
    "Residual analysis is an important procedure to validate the goodness of a linear model.\n",
    "The residual analysis could reveal a pattern in the residual charts, and this could mean several things:\n",
    "\n",
    "* we're ignoring a covariate that needs to be introduced in the model\n",
    "* we're ignoring a hidden relation with a covariate we ignore\n",
    "* data are not homoscedastic\n",
    "* data are not in a linear relation, maybe they need a transformation\n",
    "* we're missing an interaction of variables inside the model\n",
    "\n",
    "Residuals must not have a pattern themselves (autocorrelation) and should be normally distributed\n",
    "Residuals must not be related in some ways to a variable.\n",
    "\n",
    "### RSS\n",
    "\n",
    "The method of determination of the coefficients of a linear model is minimizing the residual sum of squares.\n",
    "A residual is the diff from the observed y and the predicted y hat.\n",
    "The sum of squares indicates a positive quantity that gives us the idea of how the model fits the reality.\n",
    "Conceptually, the RSS will never be 0. RSS is not useful itself, but can give us the idea that a model fits better than another if RSS is lower.\n",
    "\n",
    "### R squared\n",
    "\n",
    "Coefficient of determination. It's a ratio (0 to 1) and gives us the idea of how the model is explaining reality.\n",
    "It's the ratio between the explained deviance divided by the total deviance.\n",
    "R^2 is 74.3%. The variation in Y is explained by 74.3% by using X.\n",
    "R squared is very important for prediction, not so much for explaining relations.\n",
    "Better to use adjusted version, because too many predictors could introduce noise.\n",
    "When performing a linear regression on many variables, whenever we add a variable to the model the R^2 increases, even if the variable does not better the model. \n",
    "\n",
    "### Standard Error\n",
    "\n",
    "Average distance of the observed values from the model line.\n",
    "It goes from 0 (never) to + infinity and it's expressed in the units of Y.\n",
    "When we predict something we're more interested in SE rather than R^2 because we understand how big are the mistake we can make in terms of units.\n",
    "\n",
    "### Multi-Collinearity\n",
    "\n",
    "It is a phenomenon that happens when 2 or more predictors are not completely independent, so they are in a relation of some kind.\n",
    "The main effect is that the extended model has some additional noise inside, the coefficient can differ from the base model. Sometimes the R^2 is even higher because the model fits better, but the statistical significance of the covariates is lower.\n",
    "The most common and easy solution is to try to remove one or more covariates that are multicollinear. In this way, the model could fit better the reality. Even if, in some prediction models, small multicollinear predictors are left.\n",
    "\n",
    "### Anova\n",
    "\n",
    "Anova is a function that performs the analysis of variances. It can be used to compare two different nested models together. In this case, we perform an F statistical test. The null hypothesis is that the two model don't differ each others, and therefore, the new covariates are not significantly useful.\n",
    "In case P-value is very high, we fail to reject the null hypothesis. When P-value is low, we can reject the null.\n",
    "\n",
    "### Error Types\n",
    "\n",
    "The aim of a statistical test is trying to reject a null hypothesis or the alternative hypothesis.\n",
    "There can happen different kind of error because we're dealing with a sample of data and hence, with uncertainty.\n",
    "First type error is to reject the null hypothesis when it is not the case.\n",
    "Second type error is to fail to reject the null hypothesis when we should reject it.\n",
    "\n",
    "## LDA\n",
    "\n",
    "Linear Discriminant Analysis is a technique of dimensional reduction (thus similar to PCA).\n",
    "It creates a linear combination of variables wrt known categories. These new combinations are called Linear Discriminant LD1, LD2...\n",
    "For example, imagine we have 2 dimensions and we see some blue/red points. We could create a line/axis that maximize the blue/red points separation.\n",
    "In practical terms, we maximize the mean separations and minimize the blue/red scatter.\n",
    "This new line is the LD1 and it's useful to reduce dimensions. It's also useful for categorical predictions: given X and Y, is this point blue or red?\n",
    "\n",
    "## PCA\n",
    "\n",
    "PCA is a technique of dimensional reduction.\n",
    "We aim to identify a linear combination of variables that captures most of the variability of the variables. It is PC1.\n",
    "A linear combination is a cocktail of covariates in different ratio (loading scores).\n",
    "E.g. X,Y,Z. PC1: 0.97X + 0.2Y + 0.3Z. It means that 97% of X explains PC1 separation\n",
    "\n",
    "Through PCA we can plot 1000 dimensions onto 2 axes.\n",
    "With a biplot, we can also overlap info about covariates and which covariate is responsible for that PC axis.\n",
    "\n",
    "## Model selection\n",
    "\n",
    "* RSS - of course the lowest the better, but many times, more complex model have lowest RSS because they tend to use noise to overfit\n",
    "* Anova if models are nested\n",
    "* Adjusted R^2\n",
    "* CV - K-fold or LOOCV (number of folding). We can calculate the error of our model estimating a different dataset of data (based on CV). And we get an error. We compare 2 models errors. If delta is positive E2-E1, then it's better model M1 and viceversa.\n",
    "* AIC - Akaike information criterion, it's a way to estimate the goodness of a model from the # of independent variable and how well the model describes data (max likelihood). More Complex Models are penalized. The lowest AIC, the better. R^2 can be similar for 2 models, use AIC!\n",
    "* BIC - Bayesian I.C. like AIC, the lower, the better. AIC or BIC? Most of the time they agree. Use both to support evidence and in case they differ, report it.\n",
    "\n",
    "## Ridge and lasso\n",
    "\n",
    "We build a linear model. If the training set is small, we risk over-fitting it using least square method. e.g. with 2 points!\n",
    "We can solve the problem, by minimizing the ResidualSS + lambda * slope^2. Where lambda is a penalty. In this case, we're doing a ridge regression. lambda = 0 => un-penalized normal linear model, lambda = inf => max penalty, model has a slope=0.\n",
    "How to choose lambda? using CV!\n",
    "Lasso is similar to ridge, always same penalty mechanism, but the diff is that we drop useless covariates by setting their coef to zero. Useful in model selection, where we have many covariates.\n",
    "But ridge give of course better results.\n",
    "\n",
    "## Steps of analysis\n",
    "\n",
    "1. EDA\n",
    "   1. Read carefully the scientific question we want to answer and read the data set variables descriptions\n",
    "   2. Is the response variable a factor? Is it a continuous variable?\n",
    "   3. Is the response variable normally distributed? If not, and the observations are few, it's OK, justify it saying this. If not, apply a log transformation. Plot hist of before and after to justify that transformation.\n",
    "   4. Are there any N/A values? Remove those records\n",
    "   5. Are there any factors which are not factor? Convert them.\n",
    "   6. Does the scientific question ask us to shrink the data set? Create a new one from the old one.\n",
    "   7. Plot response variable wrt factor variables using box plots. Plot interactions between factor variables (if more than 1)\n",
    "   8. Use pairs to investigate correlations between variables or plot scatter chart between response variable and continuous variables (by hand)\n",
    "   9. Write down if you notice any linear/quadratic/non-linear trend\n",
    "2. Find a model with a subset of variables for a continuous response variable\n",
    "   1. Use standard LM\n",
    "      1. Create a model with all the possible interactions\n",
    "      2. Delete the least significant from the lm\n",
    "      3. If in doubt between 2 nested models, try to use ANOVA\n",
    "      4. Try to add polynomials\n",
    "      5. Try to add natural splines on least significative variables (clouds of points)\n",
    "   2. Extend the LM using gam\n",
    "      1. Set a seed and write it in the report\n",
    "      2. Install and import gam\n",
    "      3. Get DoF using cross validation of a smooth spline\n",
    "      4. create a GAM with that smooth spline with rounded DoF\n",
    "      5. Do it for the different variables\n",
    "   3. Validate the model\n",
    "      1. Perform residual analysis\n",
    "         1. Plot the residuals\n",
    "         2. Are the residuals equally distributed?\n",
    "         3. Are there any leverage points? Possible outliers?\n",
    "             1. In case of outlier, remove them\n",
    "             2. Re-calculate models with new dataset\n",
    "      2. Plot the predicted vs observed values with red bisector line\n",
    "      3. Comment the result\n",
    "      4. Answer the scientific question\n",
    "   4. Discriminate between models\n",
    "      1. Are you still in doubt between models?\n",
    "      2. Use `anova` if nested models\n",
    "      3. Use deviance(model) to calculate the explained deviance. Higher is better\n",
    "      4. Use extractAIC(model) to calculate the Akaike Inf Criterion. Lower is better\n",
    "      5. Compare graphically the two observed vs predicted plots\n",
    "      6. Choose the simpler one\n",
    "3. Find a model with many variables for a continuous response variable\n",
    "   1. Perform PCA - Principal Component Analysis\n",
    "      1. Apply func `prcomp`\n",
    "      2. Plot PCs using `biplot`\n",
    "      3. Do you see any horizontal, vertical vectors?\n",
    "      4. Are point separated into clouds? Maybe adding colors according to classes?\n",
    "      5. How much deviance is explained by the first N PC? Plot it\n",
    "   2. Perform PCR - Principal Component Regression\n",
    "      1. Set seed\n",
    "      2. Apply `pcr` func\n",
    "      3. Check deviance explained by the first N PCs\n",
    "      4. Plot the predicted vs observed\n",
    "   3. Use regularization\n",
    "      1. Set seed\n",
    "      2. Identify X and y (project on dataset columns)\n",
    "      3. Import glmnet lib\n",
    "      4. Ridge (best for predictions, no variable selection)\n",
    "         1. Apply `glmnet` func with alpha=0\n",
    "         2. Apply `cv.glmnet` func with alpha=0 using lambda.min\n",
    "         3. Apply `cv.glmnet` func with alpha=0 using best.lambda\n",
    "      5. Lasso (slightly worse for predictions, good for variable selection)\n",
    "         1. Same as ridge with alpha=1\n",
    "         2. Check `coef` to analyze the variables selected\n",
    "      6. Plot error curves for cv\n",
    "4. Find a model for a binary or discrete response variable (classification)\n",
    "   1. Analysis\n",
    "      1. Is the response variable binary/dycotomic or discrete?\n",
    "      2. Are the covariates few or many?\n",
    "   2. Modelling binary classification (yes vs no)\n",
    "      1. Try to use the glm using family=binomial\n",
    "      2. Try to add interactions and polynomials as you'd do with linear model\n",
    "      3. Compare models with `anova` (`chisq` test)\n",
    "      4. If in doubt use the simpler one\n",
    "      5. If some variables are not normally distributed or see clouds of points, use GAM with splines\n",
    "      6. Apply `gam` function passing `s(Variable, Degree)`\n",
    "      7. If in doubt between models check `deviance(model)` or `extractAIC(model)`\n",
    "   3. Validate binary classification\n",
    "      1. Assumption: Only 2 Classes (yes/no)\n",
    "      2. Split training/test data set\n",
    "      3. Apply model to the test set\n",
    "      4. Calculate yes/no vs predicted yes/no with a misclassification table (confusion matrix)\n",
    "      5. Install package and lib pROC\n",
    "      6. Plot ROC curve and discuss it\n",
    "      7. Check and discuss sensitivity and specificity\n",
    "   4. Modelling with discriminant analysis\n",
    "      1. Assumption: Classes are well-separated (no overlapping)\n",
    "      2. Install MASS package\n",
    "      3. Apply `LDA` func\n",
    "      4. Plot LDA model and analyze it\n",
    "         1. Are classes vertically separated? --> LD1 better\n",
    "         2. Are classes horizontally separated? --> LD2 better\n",
    "      5. Plot `ldahist`\n",
    "      6. Apply `QDA` func\n",
    "      7. Check differences with LDA\n",
    "      8. In doubt choose LDA because it's simpler\n",
    "      9. Standard Error is very high? --> Classes are not well-separated? Try something else\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cf157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
